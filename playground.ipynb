{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGLM2-6B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前置依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/lpc/models/chatglm3-6b/\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"/home/lpc/models/chatglm3-6b/\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()\n",
    "history_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response, history in model.stream_chat(tokenizer, \"你的任务是什么?\", history=history_):\n",
    "    history_ = history\n",
    "    print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCGPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from main.predictor.chatglm import GPTPredict\n",
    "\n",
    "predictor = GPTPredict(model_name=\"ChatGLM2-6B\", model_from_pretrained=\"model/chatglm3-6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predictor(\"你好?\", history=[])\n",
    "print(res['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in predictor.predict_stream(\"你的任务是什么?\", history=[]):\n",
    "    sys.stdout.write('\\r' + res['data'])\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./a.txt', encoding='utf-8') as f:\n",
    "    ask_content = f.read()\n",
    "res = predictor(ask_content, history=[])\n",
    "print(res['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGLM_LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from main.trainer.chatglm_lora import Trainer\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/lpc/models/chatglm3-6b/\", trust_remote_code=True)\n",
    "config = AutoConfig.from_pretrained(\"/home/lpc/models/chatglm3-6b/\", trust_remote_code=True)\n",
    "trainer = Trainer(tokenizer=tokenizer, config=config, from_pretrained='/home/lpc/models/chatglm3-6b/', loader_name='ChatGLM_Chat', data_path='FDEX2', max_length=3600, batch_size=4, task_name='FDEX2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trainer(num_epochs=5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推理预测\n",
    "- 方式一: 调用原生方法Chat预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.predictor.chatglm_lora import Predictor\n",
    "\n",
    "pred = Predictor(model_from_pretrained='/home/lpc/models/chatglm3-6b/', resume_path='./save_model/FDEX2/ChatGLM_5409')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pred.chat('<rag>检索增强知识: \\n1.《政府采购代理机构管理暂行办法》(财库[2018]2号)\\n第十三条 代理机构受采购人委托办理采购事宜，应当与采购人签订委托代理协议，明确采购代理范围、权限、期限、档案保存、代理费用收取方式及标准、协议解除及终止、违约责任等具体事项，约定双方权利义务。</rag>\\n请根据以上检索增强知识回答以下问题\\n采购人委托采购代理机构代理采购项目，发布招标公告后，有权更换采购代理机构吗?', max_length=3000, history=history)\n",
    "history = result[1]\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方式二: 调用重写方法 (支持批量)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.predictor.chatglm_lora import Predictor\n",
    "\n",
    "pred = Predictor(model_from_pretrained='/home/lpc/models/chatglm3-6b/', resume_path='./save_model/FDEX2/ChatGLM_5409')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pred(['Instrcution: 请识别该商品的要素: 理光（Ricoh） M2700/M2701/2702多功能黑白激光复合机 a3复合机打印机一体机办公 M 2702(网络+双面+输稿器+7寸触屏) 官方标配\\n Answer:', '你好啊'], max_length=512, build_message=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./a.txt', encoding='utf-8') as f:\n",
    "    ask_content = f.read()\n",
    "result = pred(ask_content, max_length=512)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGLM_LoRA RAG 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建或者加载chromadb客户端\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "DB_SAVE_DIR = './chroma_data'\n",
    "DB_NAME = 'FDQA'\n",
    "N_RESULTS = 1\n",
    "\n",
    "client = chromadb.PersistentClient(DB_SAVE_DIR)\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"DMetaSoul/sbert-chinese-general-v2\")\n",
    "collection = client.get_or_create_collection(DB_NAME, embedding_function=sentence_transformer_ef, metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.predictor.chatglm_lora import Predictor\n",
    "\n",
    "pred = Predictor(model_from_pretrained='./model/chatglm3-6b', resume_path='./save_model/FDRAG/ChatGLM_44136')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = '采购人委托采购代理机构代理采购项目，发布招标公告后，有权更换采购代理机构吗?'\n",
    "res = collection.query(\n",
    "    query_texts=[user_question],\n",
    "    n_results=N_RESULTS\n",
    ")\n",
    "if len(res['metadatas'][0]) > 0:\n",
    "    distance = res['distances'][0][0]\n",
    "    if distance < 0.1:\n",
    "        clue = res['metadatas'][0][0]['clue']\n",
    "    else:\n",
    "        clue = False\n",
    "else:\n",
    "    clue = False\n",
    "if not clue:\n",
    "    rag_user_question = user_question\n",
    "else:\n",
    "    rag_user_question = f'<rag>检索增强知识: \\n{clue}</rag>\\n请根据以上检索增强知识回答以下问题\\n{user_question}'\n",
    "result = pred.chat(rag_user_question, history=history)\n",
    "history = result[1]\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGLM_LoRA_RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from main.trainer.chatglm_rlhf import Trainer\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/lpc/models/chatglm3-6b/\", trust_remote_code=True)\n",
    "config = AutoConfig.from_pretrained(\"/home/lpc/models/chatglm3-6b/\", trust_remote_code=True)\n",
    "trainer = Trainer(tokenizer=tokenizer, config=config, from_pretrained='/home/lpc/models/chatglm3-6b/', reward_from_pretrained='/home/lpc/models/text2vec-base-chinese/', loader_name='ChatGLM_RLHF', data_path='ID', max_length=1200, batch_size=2, task_name='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trainer(num_epochs=5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qianwen_LoRA\n",
    "\n",
    "运行前请参阅[Qianwen-14B-Chat-Int4](https://huggingface.co/Qwen/Qwen-14B-Chat-Int4)安装相关依赖."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.trainer.qianwen_lora import Trainer\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model/Qwen-14B-Chat-Int4\", trust_remote_code=True)\n",
    "config = AutoConfig.from_pretrained(\"model/Qwen-14B-Chat-Int4\", trust_remote_code=True)\n",
    "config.disable_exllama = True\n",
    "trainer = Trainer(tokenizer=tokenizer, config=config, from_pretrained='./model/Qwen-14B-Chat-Int4', loader_name='Qianwen_Chat', data_path='FD', max_length=512, batch_size=1, task_name='FD_Qianwen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trainer(num_epochs=5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Accelerator分布式训练加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! accelerate launch run_qianwen_lora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.predictor.qianwen_lora import Predictor\n",
    "\n",
    "pred = Predictor(model_from_pretrained='./model/Qwen-14B-Chat-Int4', resume_path='./save_model/FDQALaw_Qianwen/Qwen_39000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pred.chat('hello,我想问下中华人民共和国民法典中第三条是什么?')\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 预测文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.predictor.chatglm_lora import Predictor\n",
    "\n",
    "pred = Predictor(model_from_pretrained='./model/chatglm3-6b', resume_path='./save_model/BossCondition/ChatGLM_25108')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pred('Instrcution: 请识别该商品的要素: 理光（Ricoh） M2700/M2701/2702多功能黑白激光复合机 a3复合机打印机一体机办公 M 2702(网络+双面+输稿器+7寸触屏) 官方标配\\n Answer:', max_length=512)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('./data/Boss/BertPred/复印机_retrieved.tsv', encoding='utf-8') as f:\n",
    "    ori_list = f.read().split('\\n')\n",
    "\n",
    "if ori_list[-1] == '':\n",
    "    ori_list = ori_list[:-1]\n",
    "\n",
    "result = []\n",
    "iter = tqdm(ori_list)\n",
    "for item in iter:\n",
    "    item = item.split('\\t')\n",
    "    res = pred(f'Instrcution: 请识别该商品的要素: {item[2]}\\n Answer:', max_length=512)\n",
    "    res_item = {\n",
    "        'pred': res\n",
    "    }\n",
    "    answer_index = res.find('Answer:')\n",
    "    iter.set_postfix(pred=json.dumps(res[answer_index + 7:], ensure_ascii=False))\n",
    "    result.append(res_item)\n",
    "\n",
    "with open('./data_record/BertPred_ChatGLMLoRA/复印机.json', 'w', encoding='utf-8') as f:\n",
    "    for item in result:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推理文本\n",
    "建议采用`Predictor`中的默认方法, 以便支持批量生成."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from main.evaluation.inferences import inference_with_data_path\n",
    "from main.predictor.chatglm_lora import Predictor\n",
    "\n",
    "pred = Predictor(model_from_pretrained='/home/lpc/models/chatglm3-6b/', resume_path='./save_model/FDEX2/ChatGLM_5409')\n",
    "\n",
    "def batcher(item):\n",
    "    return pred(**item, max_length=1024, temperature=0, build_message=True)\n",
    "\n",
    "inference_with_data_path(data_path='test', batcher=batcher, save_path='./outputs.txt', batch_size=4)\n",
    "\n",
    "# 若你希望能够自行喂入数据, 也可以使用inference_with_data迭代器, 注意每一条格式为{\"query\": \"\", \"history\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VLLM加速推理\n",
    "\n",
    "目前ChatGLM在`transformers==0.45.x`上还存在bug.\n",
    "\n",
    "强烈建议在新的conda环境下安装, 目前可行的版本`transformers==4.43.2 vllm==0.5.4`\n",
    "\n",
    "同时, LoRA保存的目录下需包含`config.json`, `modeling_chatglm.py`等文件, 可以从`Chatglm`模型目录里找."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "llm = LLM(model=\"/home/lpc/models/chatglm3-6b/\", enable_lora=True, trust_remote_code=True)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "     \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]\",\n",
    "     \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)\\n\\n question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]\",\n",
    "]\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    sampling_params,\n",
    "    lora_request=LoRARequest(\"lora\", 1, './save_model/FDEX2/ChatGLM_5409')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算生成文本与参考文本的评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 单例计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.evaluation.metrics import evaluate_all_metrics\n",
    "\n",
    "# 测试示例\n",
    "reference_text = [\"I love this cat.\", \"I really love this cat.\"]\n",
    "generated_text = \"hahaha I love this cat.\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/lpc/models/chatglm3-6b/\", trust_remote_code=True)\n",
    "scores = evaluate_all_metrics(tokenizer, reference_text, generated_text, intensive=False) # 如果是中文请将intensive设置为True\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 批量计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.evaluation.metrics import evaluate_generation\n",
    "\n",
    "import json\n",
    "with open('./data_record/FDEX2/outputs.txt') as f:\n",
    "    outputs = f.readlines()\n",
    "outputs = [json.loads(item) for item in outputs]\n",
    "\n",
    "with open('./data/FD/data/ex2/dev.jsonl') as f:\n",
    "    data = f.readlines()\n",
    "data = [json.loads(item) for item in data]\n",
    "data = [item['conversations'] if 'conversations' in item else item for item in data]\n",
    "data = [[item[-1]['content']] for item in data]\n",
    "# 测试示例\n",
    "reference_text = [[item] for item in data]\n",
    "generated_text = \"hahaha I love this cat.\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/lpc/models/chatglm3-6b/\", trust_remote_code=True)\n",
    "scores = evaluate_generation(tokenizer, data[:len(outputs)], outputs, intensive=True) # 如果是中文请将intensive设置为True\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 预测商品蕴涵关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.predictor.chatglm_lora import Predictor\n",
    "\n",
    "pred = Predictor(model_from_pretrained='./model/chatglm3-6b', resume_path='./save_model/BossRTE/ChatGLM_22264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pred('Instruction: 请判断以下两个商品是否为同款商品\\nContext: Source: 联想(Lenovo）启天 M415-B114 台式计算机 I3-7100/8G/1T/无光驱/15L机箱/21.5寸显示器 5288\\nTarget: 戴尔（DELL） I3-6100 戴尔（DELL）成就3667-R1308商用台式电脑整机（i3-6100 4G 1T WIFI 蓝牙 三年上门 硬盘保留 Win10）19.5英寸 3455\\nAnswer: ', max_length=512)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STS数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('./data/Boss/RTE/dev.json', encoding='utf-8') as f:\n",
    "    ori_list = f.read().split('\\n')\n",
    "\n",
    "if ori_list[-1] == '':\n",
    "    ori_list = ori_list[:-1]\n",
    "\n",
    "iter = tqdm(ori_list)\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "for item in iter:\n",
    "    item = json.loads(item)\n",
    "    res = pred(item['context'], max_length=512)\n",
    "    res = res.split('预测结果: ')\n",
    "    if len(res) < 2:\n",
    "        res = 1\n",
    "    else:\n",
    "        res = int(res[1])\n",
    "    gold = int(item['target'].split('预测结果: ')[1])\n",
    "    if res == 1:\n",
    "        if res == gold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if res == gold:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    p = tp / (tp + fp)\n",
    "    r = tp / (tp + fn)\n",
    "    f1 = 2 * p * r / (p + r)\n",
    "    iter.set_postfix(F1=f1, p=p, r=r)\n",
    "\n",
    "print(f1, p, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全样本环境预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('./data/Boss/BertPred/复印机_retrieved.tsv', encoding='utf-8') as f:\n",
    "    ori_list = f.read().split('\\n')\n",
    "\n",
    "if ori_list[-1] == '':\n",
    "    ori_list = ori_list[:-1]\n",
    "\n",
    "iter = tqdm(ori_list)\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "current_id = 0\n",
    "current_index = 0\n",
    "for idx, item in enumerate(iter):\n",
    "    item = item.split('\\t')\n",
    "    id = item[0]\n",
    "    if id != current_id:\n",
    "        current_id = id\n",
    "        current_index = idx\n",
    "    ori_item = ori_list[current_index]\n",
    "    ori_item = ori_item.split('\\t')\n",
    "    if ori_item[2] == item[2]:\n",
    "        continue\n",
    "    if len(item) < 4:\n",
    "        item.append(1)\n",
    "    if len(ori_item) < 4:\n",
    "        ori_item.append(1)\n",
    "    res = pred(f\"Instruction: 请判断以下两个商品是否为同款商品\\nContext: Source: {ori_item[2]} {ori_item[3]}\\nTarget: {item[2]} {item[3]}\\nAnswer: \", max_length=512)\n",
    "    res = res.split('预测结果: ')\n",
    "    if len(res) < 2:\n",
    "        res = 1\n",
    "    else:\n",
    "        res = int(res[1])\n",
    "    gold = int(item[5]) if len(item) > 5 else 1\n",
    "    if res == 1:\n",
    "        if res == gold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if res == gold:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    p = 0 if tp + fp == 0 else tp / (tp + fp)\n",
    "    r = 0 if tp + fn == 0 else tp / (tp + fn)\n",
    "    f1 = 0 if p + r == 0 else 2 * p * r / (p + r)\n",
    "    iter.set_postfix(F1=f1, p=p, r=r)\n",
    "\n",
    "print(f1, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_ = tp + 988 / 2\n",
    "fp_ = fp + 52 / 2\n",
    "p = tp_ / (tp_ + fp_)\n",
    "r = tp_ / (tp_ + fn)\n",
    "f1 = 2 * p * r / (p + r)\n",
    "print(f'F1: {f1}, P: {p}, R: {r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用ChatGLM3-6B对商品进行要素抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from main.predictor.chatglm import GPTPredict\n",
    "\n",
    "predictor = GPTPredict(model_name=\"ChatGLM2-6B\", model_from_pretrained=\"model/chatglm3-6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('./data/Boss/train.json') as f:\n",
    "    ori_data = f.read().split('\\n')\n",
    "\n",
    "if ori_data[-1] == '':\n",
    "    ori_data.pop()\n",
    "\n",
    "result = []\n",
    "for item in tqdm(ori_data):\n",
    "    data = json.loads(item)\n",
    "    item_id = data['item_id']\n",
    "    context = data['context']\n",
    "    question = context.replace('\\n Answer: ', '')\n",
    "    res = predictor.generate(question, max_length=1024)\n",
    "    result.append({'item_id': item_id, 'question': question, 'answer': res})\n",
    "\n",
    "with open('./data/Boss/train_result.json', encoding='utf-8', mode='w') as f:\n",
    "    for item in result:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用ChatGLM3-6B预测商品蕴涵关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from main.predictor.chatglm import GPTPredict\n",
    "\n",
    "predictor = GPTPredict(model_name=\"ChatGLM2-6B\", model_from_pretrained=\"model/chatglm3-6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('./data/Boss/BertPred/台式计算机.tsv', encoding='utf-8') as f:\n",
    "    ori_list = f.read().split('\\n')\n",
    "\n",
    "if ori_list[-1] == '':\n",
    "    ori_list = ori_list[:-1]\n",
    "\n",
    "iter = tqdm(ori_list)\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "current_id = 0\n",
    "current_index = 0\n",
    "for idx, item in enumerate(iter):\n",
    "    item = item.split('\\t')\n",
    "    id = item[0]\n",
    "    if id != current_id:\n",
    "        current_id = id\n",
    "        current_index = idx\n",
    "    ori_item = ori_list[current_index]\n",
    "    ori_item = ori_item.split('\\t')\n",
    "    if ori_item[2] == item[2]:\n",
    "        continue\n",
    "    res = predictor.generate(f\"请判断以下两个商品是否为同款商品，直接回答“同款”或“非同款”即可。\\n文本1： {ori_item[2]} {ori_item[3]}\\n文本2： {item[2]} {item[3]}\\n回答：\", max_length=1024)\n",
    "    if '非同款' in res:\n",
    "        res = 0\n",
    "    else:\n",
    "        res = 1\n",
    "    gold = int(item[5]) if len(item) > 5 else 1\n",
    "    if res == 1:\n",
    "        if res == gold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if res == gold:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    p = 0 if tp + fp == 0 else tp / (tp + fp)\n",
    "    r = 0 if tp + fn == 0 else tp / (tp + fn)\n",
    "    f1 = 0 if p + r == 0 else 2 * p * r / (p + r)\n",
    "    iter.set_postfix(F1=f1, p=p, r=r)\n",
    "\n",
    "print(f1, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_ = tp + 988 / 2\n",
    "fp_ = fp + 52 / 2\n",
    "p = tp_ / (tp_ + fp_)\n",
    "r = tp_ / (tp_ + fn)\n",
    "f1 = 2 * p * r / (p + r)\n",
    "print(f'F1: {f1}, P: {p}, R: {r}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
