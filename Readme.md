## æœ¬é¡¹ç›®ç”¨äºLLM-PEFTå…¥é—¨ä½¿ç”¨

æœ¬é¡¹ç›®æ—¨åœ¨æä¾›ä¸€ä¸ªç®€æ˜çš„å…¥é—¨æŒ‡å—ï¼Œå¸®åŠ©ç”¨æˆ·åŸºäºä¸»æµå¤§æ¨¡å‹ï¼ˆå¦‚ Llama3ã€GLM4ã€Qwen2ï¼‰è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰è®­ç»ƒå’Œæ¨ç†ã€‚

## ğŸ› ï¸ ä¸€ã€ç¯å¢ƒé…ç½®

### âœ… å®‰è£…ä¾èµ–

#### - æ”¯æŒ Llama3ã€GLM4ã€Qwen2 æ¨¡å‹ï¼š

```bash
pip install protobuf transformers>=4.44.1 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate
```

> ChatGLM3-6Bå’ŒGLM4å¯èƒ½å­˜åœ¨`transformers`ç‰ˆæœ¬é™åˆ¶, æ³¨æ„é™çº§.

---

### âœ… vLLM æ¨ç†åŠ é€Ÿï¼ˆæ¨èï¼‰

å»ºè®®ä½¿ç”¨ vLLM ä»¥æå‡æ¨ç†æ•ˆç‡ã€‚

#### - åˆ›å»º Conda ç¯å¢ƒï¼š

```bash
conda create -n vllm python=3.12 -y
conda activate vllm
```

#### - å®‰è£… vLLMï¼ˆéœ€ CUDA >= 12.1ï¼‰ï¼š

```bash
pip install vllm
```

æ›´å¤šå®‰è£…æ–¹å¼è¯¦è§ [vLLM å®˜ç½‘](https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html)

---

## âœ¨ äºŒã€æ¨¡å‹æ¨ç†ä½¿ç”¨è¯´æ˜

### 1. ä½¿ç”¨ vLLM è¿›è¡Œæ¨ç†ï¼š

```python
from main.predictor.vllm import Predictor

pred = Predictor(model_from_pretrained='Qwen/Qwen3-8B')
result = pred('é‡‡è´­äººå§”æ‰˜é‡‡è´­ä»£ç†æœºæ„ä»£ç†é‡‡è´­é¡¹ç›®ï¼Œå‘å¸ƒæ‹›æ ‡å…¬å‘Šåï¼Œæœ‰æƒæ›´æ¢é‡‡è´­ä»£ç†æœºæ„å—?', max_new_tokens=512)
print(result)
```

#### - æ”¯æŒVLæ¨¡å‹æ¨ç†

```python
pred([{
    "role": "user",
    "content": [
        {"type": "image", "image": "./example.jpg"},
        {"type": "text", "text": "å¥¹æ˜¯è°?"},
    ]
}, {
    "role": "user",
    "content": [
        {"type": "image", "image": "./example.jpg"},
        {"type": "text", "text": "å¥¹æœ‰å“ªäº›è‘—åä½œå“?"},
    ]
}])
```

é’ˆå¯¹ä¸åŒæ¨¡å‹, è¯·åœ¨[vLLM æ–‡æ¡£](https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language.html)ä¸Šè¯¦è§å‚æ•°é…ç½®, å¹¶ç›´æ¥åœ¨Predictorä¸­è®¾ç½®.

---

### 2. é¡¹ç›®å°è£…æ¨ç†è°ƒç”¨

#### - ç¤ºä¾‹ï¼šLLMæ¨ç†

```python
from main.predictor.llm import Predictor

predictor = Predictor(model_from_pretrained="model/chatglm3-6b")
res = predictor("ä½ å¥½?", history=[])
print(res)
```

- history: historyä¸ºäºŒç»´æ•°ç»„, å…¶ä¸­æ¯ä¸€é¡¹å¯¹åº”ä¸€ä¸ª`query`çš„`history`.

#### - æ”¯æŒVLæ¨¡å‹æ¨ç†

```python
pred([{
    "role": "user",
    "content": [
        {"type": "image", "image": "./example.jpg"},
        {"type": "text", "text": "å¥¹æ˜¯è°?"},
    ]
}, {
    "role": "user",
    "content": [
        {"type": "image", "image": "./example.jpg"},
        {"type": "text", "text": "å¥¹æœ‰å“ªäº›è‘—åä½œå“?"},
    ]
}])
```

å…¶ä¸­, å¯¹äºå¤§å°ºå¯¸å›¾ç‰‡éœ€æŒ‡å®šå…¶æœ€å¤§åƒç´ , è®¾ç½®æ–¹æ³•å¦‚ä¸‹ (ä»¥Qwen2.5-VLä¸ºä¾‹, `æœ€å¤§åƒç´ N`ä¸€èˆ¬è®¾ä¸º`N*28*28`):

```python
# min_pixels and max_pixels
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "file:///path/to/your/image.jpg",
                "resized_height": 280,
                "resized_width": 420,
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
# resized_height and resized_width
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "file:///path/to/your/image.jpg",
                "min_pixels": 50176,
                "max_pixels": 50176,
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
```

#### - æ”¯æŒæµå¼æ¨ç†ï¼š

```python
for res in predictor.predict_stream("ä½ çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆ?", history=[]):
    print(res[1])
```

---

### 3. LoRA å¾®è°ƒæ¨¡å‹æ¨ç†

#### - ç¤ºä¾‹ï¼šChatGLM LoRA æ¨ç†

```python
from main.predictor.llm import Predictor

predictor = Predictor(model_from_pretrained="model/chatglm3-6b", peft_path='<PEFT_PATH>')
result = pred('é‡‡è´­äººå§”æ‰˜é‡‡è´­ä»£ç†æœºæ„ä»£ç†é‡‡è´­é¡¹ç›®ï¼Œå‘å¸ƒæ‹›æ ‡å…¬å‘Šåï¼Œæœ‰æƒæ›´æ¢é‡‡è´­ä»£ç†æœºæ„å—?', max_new_tokens=512)
print(result)
```

---

### 4. Transformers å®˜æ–¹æ¨ç†æ–¹æ³•

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("model/chatglm3-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("model/chatglm3-6b", trust_remote_code=True).half().cuda()
model.eval()
```

#### - ç›´æ¥æ¨ç†ï¼š

```python
response, history = model.chat(tokenizer, "ä½ çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆ?", history=[])
print(response)
```

#### - æµå¼æ¨ç†ï¼š

```python
for response, history in model.stream_chat(tokenizer, "ä½ çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆ?", history=[]):
    print(response)
```

---

## ğŸ”¥ ä¸‰ã€PEFT å¾®è°ƒè®­ç»ƒ

```python
from main.trainer.llm_lora import Trainer
from transformers import AutoTokenizer, AutoConfig

tokenizer = AutoTokenizer.from_pretrained("model/chatglm3-6b", trust_remote_code=True)
config = AutoConfig.from_pretrained("model/chatglm3-6b", trust_remote_code=True)

trainer = Trainer(
    tokenizer=tokenizer,
    config=config,
    from_pretrained='./model/chatglm3-6b',
    loader_name='ChatGLM_Chat',
    data_path='<dataset_name>',
    max_new_tokens=3600,
    batch_size=1,
    task_name='<dataset_name>'
)
```

- loader_name: æ•°æ®é›†åŠ è½½å™¨, å…¶ä¸­`ChatGLM <= 3`ä¸º`ChatGLM_Chat`, å…¶ä½™å‡ä½¿ç”¨`LLM_Chat`.
- `<dataset_name>`: è¡¨ç¤ºé€‰ç”¨çš„è®­ç»ƒæ•°æ®é›†ç±»å‹, è¯·åˆ›å»º`./data/present.json`æ–‡ä»¶å¹¶è‡ªå®šä¹‰æ•°æ®é›†è·¯å¾„.

### æ•°æ®é›†é…ç½®è¯´æ˜

è¯·åœ¨ `./data/present.json` ä¸­é…ç½®è®­ç»ƒæ•°æ®è·¯å¾„ï¼š

```json
{
  "qa_dataset": {
    "train": "./data/qa_train.json",
    "dev": "./data/qa_dev.json"
  },
  "law_dataset": {
    "train": "./data/law_train.json",
    "dev": "./data/law_dev.json"
  }
}
```

å½“æ­¤æ—¶, `<dataset_name>`é€‰å–ä¸º`qa_dataset`æ—¶, æ¨¡å‹å°†è‡ªåŠ¨è¯»å–å¯¹åº”çš„`train`, `dev`å’Œ`test`(å¯ç¼ºçœ)è·¯å¾„ä¸‹çš„æ•°æ®é›†.

### æ•°æ®æ ¼å¼

* **ChatGLM\_Chat æ ¼å¼**ï¼ˆæ¨èï¼‰ï¼š

```json
{"conversations": [{"role": "user", "content": "è¯·è¯†åˆ«xxx\nè¾“å…¥: ä¸‰ä»¶äº‹ä¸èƒ½ç¡¬æ’‘"}, {"role": "assistant", "content": "å¥½çš„, ç­”æ¡ˆæ˜¯xxx"}]}
```

* **ChatGLM\_LoRA æ ¼å¼**ï¼ˆæ›´çµæ´»ï¼‰ï¼š

```json
[{"context": "Instruction: è¯·è¯†åˆ«xxx\nè¾“å…¥: ä¸‰ä»¶äº‹ä¸èƒ½ç¡¬æ’‘\nAnswer: ", "target": "å¥½çš„, ç­”æ¡ˆæ˜¯xxx\n"}]
```

---

## ğŸš€ å››ã€åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

### âœ… ä½¿ç”¨ Accelerate åˆ†å¸ƒå¼è®­ç»ƒï¼š

```bash
accelerate launch --num_processes=<n_gpu> <your_script>.py
```

> æ³¨æ„ï¼šbatch\_size è¡¨ç¤ºæ¯ä¸ª GPU ä¸Šçš„ batch å¤§å°ã€‚

---

### âœ… å¯ç”¨ DeepSpeed ZeRO-3 + å¼ é‡å¹¶è¡Œ

#### å®‰è£…ï¼š

```bash
pip install deepspeed
```

#### é…ç½®ï¼š

```bash
accelerate config
# DeepSpeed -> Yes
# DeepSpeed config file -> ./ds_config.json
```

- é…ç½®è¿‡ç¨‹ä¸­, GPUé€‰æ‹©`multi-GPU`

- æ˜¯å¦ä½¿ç”¨DeepSpeed: `Yes`

- æ˜¯å¦æŒ‡å®šDeepSpeedé…ç½®æ–‡ä»¶: `Yes`

- DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„: `./ds_config.json`

å…¶ä»–é€‰é¡¹é»˜è®¤ä¸º`No`, å³[yes/No]ä¸­ç›´æ¥å›è½¦.

`ds_config.json`é…ç½®æ–‡ä»¶å†…å®¹å¦‚ä¸‹:

ç¤ºä¾‹é…ç½®æ–‡ä»¶ `ds_config.json`ï¼š

```json
{
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 1,
    "zero_optimization": {
        "stage": 3,
        "offload_param": {
            "device": "cpu"
        }
    },
    "tensor_model_parallel_size": 2,
    "pipeline_model_parallel_size": 1,
    "fp16": {
        "enabled": true
    }
}
```

---

## ğŸ­ äº”ã€PEFT + PPO å¼ºåŒ–å­¦ä¹ å¾®è°ƒ

æœ¬é¡¹ç›®ç°å·²æ”¯æŒ`ChatGLM3`ã€`ChatGLM4`ã€`Qwen2.5`ã€`Llama3`ç­‰ç³»åˆ—çš„æ¨¡å‹è¿›è¡ŒPEFT+PPOå¾®è°ƒè®­ç»ƒï¼Œä½¿ç”¨æ—¶æ³¨æ„ä½¿ç”¨ä¸Šè¿°æ¨¡å‹å¯¹åº”çš„transformersç‰ˆæœ¬ï¼Œæ¨èä½¿ç”¨å¦‚ä¸‹ç‰ˆæœ¬ï¼š
| æ¨¡å‹ç³»åˆ—        |æ¨ètransformersç‰ˆæœ¬                      |
| -----------  | ----------------------------- |
| ChatGLM3     |  `4.40.0`   |
| ChatGLM4     |  `>=4.46.0` ï¼ˆå¦‚éœ€è¦ä½¿ç”¨`>=4.49.0`ï¼Œéœ€åˆ°[huggingface](https://huggingface.co/THUDM/glm-4-9b-chat/commit/bd8234fe5e0c09c48637a92abb0c797cb5fa0e73)ä¸Šæ›´æ–°`modeling_chatglm.py`æ–‡ä»¶ï¼‰  |
| Qwen2.5      |  `4.43.0`   |
| Llama3/3.1/3.2      |  `4.43.0`   |
```python
from main.trainer.chatglm_rlhf_base import Trainer
from transformers import AutoTokenizer, AutoConfig
import datetime

tokenizer = AutoTokenizer.from_pretrained(r"/root/autodl-tmp/models/chatglm4-9b-chat", trust_remote_code=True)
config = AutoConfig.from_pretrained(r"/root/autodl-tmp/models/chatglm4-9b-chat", trust_remote_code=True)

trainer = Trainer(tokenizer=tokenizer,
 config=config, 
 from_pretrained=r"/root/autodl-tmp/models/chatglm4-9b-chat", 
 reward_from_pretrained=r"/root/autodl-tmp/models/text2vec-base-multilingual", 
 loader_name='LLM_RLHF',
 data_path='Wiki_Humans_RL_100', 
 ratio_for_rlhf=-1.0, 
 max_length=1024, 
 batch_size=4, 
 task_name='Wiki-humans-rawrl-' + '_' + datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

for i in trainer(num_epochs=50, weight_for_cos_and_jaccard=[0.5, 0.5], ppo_epsilon=0.15, lr=5e-4, ppo_epochs=3, alpha=0.5, beta=0.5, gamma=0): 
    a = i
```

åŸºæœ¬è®¾ç½®ï¼š
- `reward_from_pretrained`: Reward Modelæ¨¡å‹æ–‡ä»¶ï¼Œåœ¨æœ¬é¡¹ç›®ä¸­ä½¿ç”¨è½»ä¾¿ã€èƒ½å‡†ç¡®åˆ†è¯çš„éé€šç”¨æ¨¡å‹å³å¯å®ç°è®­ç»ƒï¼ˆå¦‚`text2vec`ã€`qwen3-embedding`ç­‰ï¼‰
- `loader_name`: æ•°æ®åŠ è½½å™¨çš„åç§°ï¼Œå¯åœ¨`main/loaders.py`ä¸‹æŸ¥çœ‹å½“å‰æ”¯æŒçš„æ•°æ®å½¢å¼
- `data_path`: æ•°æ®åœ°å€ï¼Œä½ éœ€è¦å…ˆåˆ›å»ºä¸€ä¸ª`present.json`ï¼Œåœ¨è¯¥æ–‡ä»¶ä¸‹è¿›è¡Œè·¯å¾„æŒ‡å®šï¼Œå…·ä½“æ“ä½œæ–¹æ³•å‰é¢å·²æåˆ°ï¼Œæ³¨æ„ï¼Œä½ éœ€è¦åˆ°`loaders.py`ä¸­å°†`data_path`ä¿®æ”¹å­˜æ”¾`present.json`çš„ä½ç½®
- `ratio_for_rlhf`: è¿›è¡Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ¦‚ç‡ï¼Œå¯ä»¥è®¾ç½®ä¸ºå®Œå…¨åœ¨çº¿å­¦ä¹ (=1)ï¼Œæˆ–å®Œå…¨ç¦»çº¿å­¦ä¹ (<=0)
- `actor_resume_path`: ç­–ç•¥æ¨¡å‹é¢„è®­ç»ƒæ–‡ä»¶(å¯é€‰)
- `critic_resume_path`: è¯„è®ºå®¶æ¨¡å‹é¢„è®­ç»ƒæ–‡ä»¶(å¯é€‰)

å…¶ä»–å…³é”®è®¾ç½®ï¼š
- `weight_for_cos_and_jaccard`: æƒé‡çŸ©é˜µï¼Œåˆ†é…å¥–åŠ±åˆ†æ•°ä¸­cosç›¸ä¼¼åº¦æŒ‡æ ‡ä¸jaccardç›¸ä¼¼åº¦æŒ‡æ ‡çš„æƒé‡ï¼Œæ³¨æ„äºŒè€…ä¹‹å’Œè¦ä¸º1ï¼›å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œå¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚åœ¨`model`ä¸‹ä¿®æ”¹å¥–åŠ±åˆ†é…æ–¹æ³•
- `ppo_epsilon`: PPOè£åˆ‡ç³»æ•°
- `ppo_epoch`: æŒ‡å®šé‡è¦æ€§é‡‡æ ·æ¬¡æ•°ï¼Œä¹Ÿå°±æ˜¯å‚è€ƒæ¨¡å‹éœ€è¦åœ¨ç­–ç•¥æ¨¡å‹æ›´æ–°å¤šå°‘æ¬¡åè¿›è¡Œæ›´æ–°
- `alpha`ã€`beta`ã€`gamma`:åˆ†åˆ«ç¡®å®šPPOæŸå¤±å¼ä¸­ï¼Œç­–ç•¥æ¨¡å‹æŸå¤±ã€è¯„è®ºå®¶æ¨¡å‹æŸå¤±ã€ç†µçš„æƒé‡

æ•°æ®æ ¼å¼ï¼š

- `LLM_RLHF`: è®­ç»ƒæ•°æ®é›†æ ¼å¼åŒ…å«`conversations`, `gold_answers`å’Œ`bad_answers`ä¸‰ä¸ªå­—æ®µ.

```json
{
  "conversations": [...],
  "gold_answers": ["ç†æƒ³ç­”æ¡ˆ"],
  "bad_answers": ["é”™è¯¯ç­”æ¡ˆ1", "é”™è¯¯ç­”æ¡ˆ2"]
}
```

- PPOå¯¹å‚æ•°æä¸ºæ•æ„Ÿï¼Œä¸”å¯è°ƒèŠ‚çš„å‚æ•°æ•°é‡è¾ƒå¤šï¼Œå› æ­¤è®­ç»ƒæ—¶è¦å¤šæ¬¡å°è¯•ï¼Œæ‰¾åˆ°è¡¨ç°è¾ƒå¥½çš„å‚æ•°ç»„åˆ

- ä¸ºäº†æ–¹ä¾¿å¯¹è®­ç»ƒè¿›è¡Œç›‘æ§ï¼ŒPPOè®­ç»ƒå¼•å…¥äº†`tensorboard`è¿›è¡Œæ€§èƒ½ç›‘æ§ï¼Œå¯ä»¥é€šè¿‡`tensorboard`é¢æ¿æŸ¥çœ‹å½“å‰æ¨¡å‹çš„è®­ç»ƒæƒ…å†µ
    - é¦–å…ˆå®‰è£…`tensorboard`
    ```bash
        pip install tensorboard
    ```
    - å¼€å§‹è®­ç»ƒåï¼Œå¯åœ¨ç»ˆç«¯ä½¿ç”¨å¦‚ä¸‹æŒ‡ä»¤æ‰“å¼€`tensorboard`é¢æ¿
    ```bash
        tensorboard --logdir={your_saved_dir} [--port=xx]
    ```
    å…¶ä¸­`--logdir`æ˜¯ä½ ä¿å­˜çš„`tensorboard`æ–‡ä»¶çš„åœ°å€ï¼Œæœ¬é¡¹ç›®é»˜è®¤ä¿å­˜åœ¨`logs/tensorboard_logs`ä¸‹ï¼›`--port`å¯ä»¥æŒ‡å®šé¢æ¿åŠ è½½çš„ç«¯å£ï¼Œè‹¥ä¸æŒ‡å®šï¼Œé»˜è®¤åœ¨`localhost:6006`ä¸Šæ‰“å¼€ã€‚
    - `tensorboard`ä»…åœ¨å•å¡è®­ç»ƒæ—¶ä¼šæœ‰æ¯”è¾ƒç›´è§‚çš„ç›‘æ§æ•ˆæœï¼Œå¤šå¡æ—¶æ›²çº¿ä¼šé‡å ï¼Œå»ºè®®è·‘å¤šå¡å‰å…ˆåœ¨å•å¡ä¸Šç”¨`tensorboard`çœ‹ä¸€ä¸‹æ•ˆæœï¼Œç„¶åå†åœ¨å¤šå¡ä¸Šæ­£å¼è·‘

- è‹¥ä½ æœ‰æ–°çš„æƒ³æ³•ï¼Œéœ€è¦å¯¹ä¸Šè¿°æ•°æ®æ ¼å¼è¿›è¡Œä¿®æ”¹ï¼Œå¹¶ä¿®æ”¹`loaders`ã€`models`ã€`trainers`ä¸‹çš„æ–‡ä»¶ï¼Œä»¥é€‚é…ä½ çš„è®¾å®š

---

## ğŸ’­ å…­ã€RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ¨ç†

ä½¿ç”¨å‰, éœ€å®‰è£…å¥½`chromadb`

```bash
pip install chromadb
```

### âœ… æ„å»º chromadb æ£€ç´¢æ•°æ®åº“

```python
import chromadb
from chromadb.utils import embedding_functions

DB_SAVE_DIR = './æ•°æ®åº“ç›®å½•'
DB_NAME = 'è¯»å–çš„æ•°æ®åº“åç§°'
N_RESULTS = 1

client = chromadb.PersistentClient(DB_SAVE_DIR)
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="DMetaSoul/sbert-chinese-general-v2")
collection = client.get_or_create_collection(DB_NAME, embedding_function=sentence_transformer_ef, metadata={"hnsw:space": "cosine"})
```

### âœ… å¯ç”¨ RAG æ¨ç†ï¼š

```python
from main.predictor.llm import Predictor

pred = Predictor(model_from_pretrained='./model/chatglm3-6b', peft_path='./save_model/RAG/ChatGLM_44136')

user_question = 'è¿™é‡Œæ˜¯ç”¨æˆ·çš„æé—®'
# æ£€ç´¢ç›¸å…³ç‰‡æ®µ
res = collection.query(
    query_texts=[user_question],
    n_results=N_RESULTS
)
# æ ¹æ®è·ç¦»åˆ¤æ–­æ˜¯å¦å¼•ç”¨æ£€ç´¢ä¿¡æ¯, å¦‚æœæ£€ç´¢ç‰‡æ®µä¸ç›®æ ‡è·ç¦»è¿‡å¤§(å³ç›¸å…³æ€§ä½), åˆ™ä¸ä½¿ç”¨clue
if len(res['metadatas'][0]) > 0:
    distance = res['distances'][0][0]
    if distance < 0.1:
        clue = res['metadatas'][0][0]['clue']
    else:
        clue = False
else:
    clue = False
if not clue:
    rag_user_question = user_question
else:
    rag_user_question = f'<rag>æ£€ç´¢å¢å¼ºçŸ¥è¯†: \n{clue}</rag>\nè¯·æ ¹æ®ä»¥ä¸Šæ£€ç´¢å¢å¼ºçŸ¥è¯†å›ç­”ä»¥ä¸‹é—®é¢˜\n{user_question}'
# æ‹¼æ¥å¥½çº¿ç´¢åè¿›è¡Œæé—®
result = pred(rag_user_question, history=history)
print(result[0])
```

---

## â« ä¸ƒã€è¾…åŠ©çš„éªŒè¯é›†/æµ‹è¯•é›†æ‰¹é‡æ¨ç†

```python
from main.evaluation.inferences import inference_with_data_path
from main.predictor.llm import Predictor

pred = Predictor(model_from_pretrained='/home/lpc/models/chatglm3-6b/', peft_path='./save_model/ChatGLM_LoRA')

def batcher(item):
    return pred(**item, max_new_tokens=1024, temperature=0, build_message=True)

inference_with_data_path(data_path='YOUR_PATH', batcher=batcher, save_path='./outputs.txt', batch_size=4)
```

---

## ğŸ§ª å…«ã€è¯„ä¼°æŒ‡æ ‡

### - å•æ¡æ–‡æœ¬è¯„ä¼°ï¼š

```python
from main.evaluation.metrics import evaluate_all_metrics

# æµ‹è¯•ç¤ºä¾‹
reference_text = ["I love this cat.", "I really love this cat."]
generated_text = "hahaha I love this cat."

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/home/lpc/models/chatglm3-6b/", trust_remote_code=True)
scores = evaluate_all_metrics(tokenizer, reference_text, generated_text, intensive=False) # å¦‚æœæ˜¯ä¸­æ–‡è¯·å°†intensiveè®¾ç½®ä¸ºTrue
print(scores)
```

### - æ‰¹é‡è¯„ä¼°ï¼š

```python
from main.evaluation.metrics import evaluate_generation

# æµ‹è¯•ç¤ºä¾‹
reference_text = ["I love this cat.", "I really love this cat."]
generated_text = "hahaha I love this cat."

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/home/lpc/models/chatglm3-6b/", trust_remote_code=True)
scores = evaluate_generation(tokenizer, [reference_text], [generated_text], intensive=False) # å¦‚æœæ˜¯ä¸­æ–‡è¯·å°†intensiveè®¾ç½®ä¸ºTrue
print(scores)
```

```bash
+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+--------+--------+
| Metric | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | ROUGE-1 | ROUGE-2 | ROUGE-3 | ROUGE-4 | ROUGE-L | METEOR |  TER   |
+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+--------+--------+
| Scores |  0.7   | 0.6236 | 0.5298 | 0.4518 |  0.8889 |  0.8571 |   0.8   |  0.6667 |  0.8889 | 0.958  | 0.3043 |
+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+--------+--------+

{'BLEU-1': 0.7, 'BLEU-2': 0.6236095644623235, 'BLEU-3': 0.5297521706139517, 'BLEU-4': 0.4518010018049224, 'ROUGE-1': 0.888888888888889, 'ROUGE-2': 0.8571428571428571, 'ROUGE-3': 0.8, 'ROUGE-4': 0.6666666666666666, 'ROUGE-L': 0.888888888888889, 'METEOR': 0.9579668787425148, 'TER': 0.30434782608695654}
```