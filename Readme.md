## æœ¬é¡¹ç›®ç”¨äºLLM-PEFTå…¥é—¨ä½¿ç”¨

æœ¬é¡¹ç›®æ—¨åœ¨æä¾›ä¸€ä¸ªç®€æ˜çš„å…¥é—¨æŒ‡å—ï¼Œå¸®åŠ©ç”¨æˆ·åŸºäºä¸»æµå¤§æ¨¡å‹ï¼ˆå¦‚ Llama3ã€GLM4ã€Qwen2ï¼‰è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰è®­ç»ƒå’Œæ¨ç†ã€‚

## ğŸ› ï¸ ä¸€ã€ç¯å¢ƒé…ç½®

### âœ… å®‰è£…ä¾èµ–

#### - æ”¯æŒ Llama3ã€GLM4ã€Qwen2 æ¨¡å‹ï¼š

```bash
pip install protobuf transformers>=4.44.1 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate
```

#### - è‹¥ä½¿ç”¨ GLM3 æ¨¡å‹ï¼š

```bash
pip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate
```

---

### âœ… vLLM æ¨ç†åŠ é€Ÿï¼ˆæ¨èï¼‰

å»ºè®®ä½¿ç”¨ vLLM ä»¥æå‡æ¨ç†æ•ˆç‡ã€‚

#### - åˆ›å»º Conda ç¯å¢ƒï¼š

```bash
conda create -n vllm python=3.12 -y
conda activate vllm
```

#### - å®‰è£… vLLMï¼ˆéœ€ CUDA >= 12.1ï¼‰ï¼š

```bash
pip install vllm
```

æ›´å¤šå®‰è£…æ–¹å¼è¯¦è§ [vLLM å®˜ç½‘](https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html)

---

## âœ¨ äºŒã€æ¨¡å‹æ¨ç†ä½¿ç”¨è¯´æ˜

### 1. ä½¿ç”¨ vLLM è¿›è¡Œæ¨ç†ï¼š

```python
from main.predictor.vllm import Predictor

pred = Predictor(model_from_pretrained='Qwen/Qwen3-8B')
result = pred('é‡‡è´­äººå§”æ‰˜é‡‡è´­ä»£ç†æœºæ„ä»£ç†é‡‡è´­é¡¹ç›®ï¼Œå‘å¸ƒæ‹›æ ‡å…¬å‘Šåï¼Œæœ‰æƒæ›´æ¢é‡‡è´­ä»£ç†æœºæ„å—?', max_new_tokens=512)
print(result)
```

---

### 2. é¡¹ç›®å°è£…æ¨ç†è°ƒç”¨

æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¯¹åº”æ¨¡å—ï¼š

| æ¨¡å‹ç±»å‹        | ä½¿ç”¨æ¨¡å—                     |
| ----------- | ------------------------ |
| ChatGLMï¼ˆâ‰¤3ï¼‰ | `main.predictor.chatglm` |
| å…¶ä»–          | `main.predictor.llm`     |

#### - ç¤ºä¾‹ï¼šChatGLM æ¨ç†

```python
from main.predictor.chatglm import Predictor

predictor = Predictor(model_name="ChatGLM2-6B", model_from_pretrained="model/chatglm3-6b")
res = predictor("ä½ å¥½?", history=[])
print(res)
```

#### - æ”¯æŒæµå¼æ¨ç†ï¼š

```python
for res in predictor.stream_chat("ä½ çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆ?", history=[]):
    sys.stdout.write('\r' + res[0])
    sys.stdout.flush()
```

---

### 3. LoRA å¾®è°ƒæ¨¡å‹æ¨ç†

| æ¨¡å‹ç±»å‹        | ä½¿ç”¨æ¨¡å—                          |
| ----------- | ----------------------------- |
| ChatGLMï¼ˆâ‰¤3ï¼‰ | `main.predictor.chatglm_lora` |
| å…¶ä»–          | `main.predictor.llm_lora`     |

#### - ç¤ºä¾‹ï¼šChatGLM LoRA æ¨ç†

```python
from main.predictor.chatglm_lora import Predictor

pred = Predictor(model_from_pretrained='./model/chatglm3-6b', resume_path='./save_model/RAG/ChatGLM_44136')
result = pred('é‡‡è´­äººå§”æ‰˜é‡‡è´­ä»£ç†æœºæ„ä»£ç†é‡‡è´­é¡¹ç›®ï¼Œå‘å¸ƒæ‹›æ ‡å…¬å‘Šåï¼Œæœ‰æƒæ›´æ¢é‡‡è´­ä»£ç†æœºæ„å—?', max_new_tokens=512)
print(result)
```

---

### 4. Transformers å®˜æ–¹æ¨ç†æ–¹æ³•

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("model/chatglm3-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("model/chatglm3-6b", trust_remote_code=True).half().cuda()
model.eval()
```

#### - ç›´æ¥æ¨ç†ï¼š

```python
response, history = model.chat(tokenizer, "ä½ çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆ?", history=[])
print(response)
```

#### - æµå¼æ¨ç†ï¼š

```python
for response, history in model.stream_chat(tokenizer, "ä½ çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆ?", history=[]):
    print(response)
```

---

## ğŸ”¥ ä¸‰ã€PEFT å¾®è°ƒè®­ç»ƒ

```python
from main.trainer.llm_lora import Trainer
from transformers import AutoTokenizer, AutoConfig

tokenizer = AutoTokenizer.from_pretrained("model/chatglm3-6b", trust_remote_code=True)
config = AutoConfig.from_pretrained("model/chatglm3-6b", trust_remote_code=True)

trainer = Trainer(
    tokenizer=tokenizer,
    config=config,
    from_pretrained='./model/chatglm3-6b',
    loader_name='ChatGLM_Chat',
    data_path='<dataset_name>',
    max_new_tokens=3600,
    batch_size=1,
    task_name='<dataset_name>'
)
```

- loader_name: æ•°æ®é›†åŠ è½½å™¨, å…¶ä¸­`ChatGLM <= 3`ä¸º`ChatGLM_Chat`, å…¶ä½™å‡ä½¿ç”¨`LLM_Chat`.
- `<dataset_name>`: è¡¨ç¤ºé€‰ç”¨çš„è®­ç»ƒæ•°æ®é›†ç±»å‹, è¯·åˆ›å»º`./data/present.json`æ–‡ä»¶å¹¶è‡ªå®šä¹‰æ•°æ®é›†è·¯å¾„.

### æ•°æ®é›†é…ç½®è¯´æ˜

è¯·åœ¨ `./data/present.json` ä¸­é…ç½®è®­ç»ƒæ•°æ®è·¯å¾„ï¼š

```json
{
  "qa_dataset": {
    "train": "./data/qa_train.json",
    "dev": "./data/qa_dev.json"
  },
  "law_dataset": {
    "train": "./data/law_train.json",
    "dev": "./data/law_dev.json"
  }
}
```

å½“æ­¤æ—¶, `<dataset_name>`é€‰å–ä¸º`qa_dataset`æ—¶, æ¨¡å‹å°†è‡ªåŠ¨è¯»å–å¯¹åº”çš„`train`, `dev`å’Œ`test`(å¯ç¼ºçœ)è·¯å¾„ä¸‹çš„æ•°æ®é›†.

### æ•°æ®æ ¼å¼

* **ChatGLM\_Chat æ ¼å¼**ï¼ˆæ¨èï¼‰ï¼š

```json
{"conversations": [{"role": "user", "content": "è¯·è¯†åˆ«xxx\nè¾“å…¥: ä¸‰ä»¶äº‹ä¸èƒ½ç¡¬æ’‘"}, {"role": "assistant", "content": "å¥½çš„, ç­”æ¡ˆæ˜¯xxx"}]}
```

* **ChatGLM\_LoRA æ ¼å¼**ï¼ˆæ›´çµæ´»ï¼‰ï¼š

```json
[{"context": "Instruction: è¯·è¯†åˆ«xxx\nè¾“å…¥: ä¸‰ä»¶äº‹ä¸èƒ½ç¡¬æ’‘\nAnswer: ", "target": "å¥½çš„, ç­”æ¡ˆæ˜¯xxx\n"}]
```

---

## ğŸš€ å››ã€åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

### âœ… ä½¿ç”¨ Accelerate åˆ†å¸ƒå¼è®­ç»ƒï¼š

```bash
accelerate launch --num_processes=<n_gpu> <your_script>.py
```

> æ³¨æ„ï¼šbatch\_size è¡¨ç¤ºæ¯ä¸ª GPU ä¸Šçš„ batch å¤§å°ã€‚

---

### âœ… å¯ç”¨ DeepSpeed ZeRO-3 + å¼ é‡å¹¶è¡Œ

#### å®‰è£…ï¼š

```bash
pip install deepspeed
```

#### é…ç½®ï¼š

```bash
accelerate config
# DeepSpeed -> Yes
# DeepSpeed config file -> ./ds_config.json
```

- é…ç½®è¿‡ç¨‹ä¸­, GPUé€‰æ‹©`multi-GPU`

- æ˜¯å¦ä½¿ç”¨DeepSpeed: `Yes`

- æ˜¯å¦æŒ‡å®šDeepSpeedé…ç½®æ–‡ä»¶: `Yes`

- DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„: `./ds_config.json`

å…¶ä»–é€‰é¡¹é»˜è®¤ä¸º`No`, å³[yes/No]ä¸­ç›´æ¥å›è½¦.

`ds_config.json`é…ç½®æ–‡ä»¶å†…å®¹å¦‚ä¸‹:

ç¤ºä¾‹é…ç½®æ–‡ä»¶ `ds_config.json`ï¼š

```json
{
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 1,
    "zero_optimization": {
        "stage": 3,
        "offload_param": {
            "device": "cpu"
        }
    },
    "tensor_model_parallel_size": 2,
    "pipeline_model_parallel_size": 1,
    "fp16": {
        "enabled": true
    }
}
```

---

## ğŸ­ äº”ã€PEFT + PPO å¼ºåŒ–å­¦ä¹ å¾®è°ƒ

```python
from main.trainer.chatglm_rlhf import Trainer

trainer = Trainer(
    tokenizer=tokenizer,
    config=config,
    from_pretrained='/home/lpc/models/chatglm3-6b/',
    reward_from_pretrained='/home/lpc/models/text2vec-base-chinese/',
    loader_name='ChatGLM_RLHF',
    data_path='ID',
    max_new_tokens=1200,
    batch_size=2,
    task_name='ID'
)

for i in trainer(num_epochs=5):
    a = i
```

- `reward_from_pretrained`: Reward Modelæ¨¡å‹æ–‡ä»¶

æ•°æ®æ ¼å¼ï¼š

- `ChatGLM_RLHF`: è®­ç»ƒæ•°æ®é›†æ ¼å¼åŒ…å«`conversations`, `gold_answers`å’Œ`bad_answers`ä¸‰ä¸ªå­—æ®µ.

```json
{
  "conversations": [...],
  "gold_answers": ["ç†æƒ³ç­”æ¡ˆ"],
  "bad_answers": ["é”™è¯¯ç­”æ¡ˆ1", "é”™è¯¯ç­”æ¡ˆ2"]
}
```

---

## ğŸ’­ å…­ã€RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ¨ç†

ä½¿ç”¨å‰, éœ€å®‰è£…å¥½`chromadb`

```bash
pip install chromadb
```

### âœ… æ„å»º chromadb æ£€ç´¢æ•°æ®åº“

```python
import chromadb
from chromadb.utils import embedding_functions

DB_SAVE_DIR = './æ•°æ®åº“ç›®å½•'
DB_NAME = 'è¯»å–çš„æ•°æ®åº“åç§°'
N_RESULTS = 1

client = chromadb.PersistentClient(DB_SAVE_DIR)
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="DMetaSoul/sbert-chinese-general-v2")
collection = client.get_or_create_collection(DB_NAME, embedding_function=sentence_transformer_ef, metadata={"hnsw:space": "cosine"})
```

### âœ… å¯ç”¨ RAG æ¨ç†ï¼š

```python
from main.predictor.chatglm_lora import Predictor

pred = Predictor(model_from_pretrained='./model/chatglm3-6b', resume_path='./save_model/RAG/ChatGLM_44136')

user_question = 'è¿™é‡Œæ˜¯ç”¨æˆ·çš„æé—®'
# æ£€ç´¢ç›¸å…³ç‰‡æ®µ
res = collection.query(
    query_texts=[user_question],
    n_results=N_RESULTS
)
# æ ¹æ®è·ç¦»åˆ¤æ–­æ˜¯å¦å¼•ç”¨æ£€ç´¢ä¿¡æ¯, å¦‚æœæ£€ç´¢ç‰‡æ®µä¸ç›®æ ‡è·ç¦»è¿‡å¤§(å³ç›¸å…³æ€§ä½), åˆ™ä¸ä½¿ç”¨clue
if len(res['metadatas'][0]) > 0:
    distance = res['distances'][0][0]
    if distance < 0.1:
        clue = res['metadatas'][0][0]['clue']
    else:
        clue = False
else:
    clue = False
if not clue:
    rag_user_question = user_question
else:
    rag_user_question = f'<rag>æ£€ç´¢å¢å¼ºçŸ¥è¯†: \n{clue}</rag>\nè¯·æ ¹æ®ä»¥ä¸Šæ£€ç´¢å¢å¼ºçŸ¥è¯†å›ç­”ä»¥ä¸‹é—®é¢˜\n{user_question}'
# æ‹¼æ¥å¥½çº¿ç´¢åè¿›è¡Œæé—®
result = pred.chat(rag_user_question, history=history)
history = result[1]
print(result[0])
```

---

## â« ä¸ƒã€è¾…åŠ©çš„éªŒè¯é›†/æµ‹è¯•é›†æ‰¹é‡æ¨ç†

```python
from main.evaluation.inferences import inference_with_data_path
from main.predictor.chatglm_lora import Predictor

pred = Predictor(model_from_pretrained='/home/lpc/models/chatglm3-6b/', resume_path='./save_model/ChatGLM_LoRA')

def batcher(item):
    return pred(**item, max_new_tokens=1024, temperature=0, build_message=True)

inference_with_data_path(data_path='YOUR_PATH', batcher=batcher, save_path='./outputs.txt', batch_size=4)
```

---

## ğŸ§ª å…«ã€è¯„ä¼°æŒ‡æ ‡

### - å•æ¡æ–‡æœ¬è¯„ä¼°ï¼š

```python
from main.evaluation.metrics import evaluate_all_metrics

# æµ‹è¯•ç¤ºä¾‹
reference_text = ["I love this cat.", "I really love this cat."]
generated_text = "hahaha I love this cat."

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/home/lpc/models/chatglm3-6b/", trust_remote_code=True)
scores = evaluate_all_metrics(tokenizer, reference_text, generated_text, intensive=False) # å¦‚æœæ˜¯ä¸­æ–‡è¯·å°†intensiveè®¾ç½®ä¸ºTrue
print(scores)
```

### - æ‰¹é‡è¯„ä¼°ï¼š

```python
from main.evaluation.metrics import evaluate_generation

# æµ‹è¯•ç¤ºä¾‹
reference_text = ["I love this cat.", "I really love this cat."]
generated_text = "hahaha I love this cat."

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/home/lpc/models/chatglm3-6b/", trust_remote_code=True)
scores = evaluate_generation(tokenizer, [reference_text], [generated_text], intensive=False) # å¦‚æœæ˜¯ä¸­æ–‡è¯·å°†intensiveè®¾ç½®ä¸ºTrue
print(scores)
```

```bash
+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+--------+--------+
| Metric | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | ROUGE-1 | ROUGE-2 | ROUGE-3 | ROUGE-4 | ROUGE-L | METEOR |  TER   |
+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+--------+--------+
| Scores |  0.7   | 0.6236 | 0.5298 | 0.4518 |  0.8889 |  0.8571 |   0.8   |  0.6667 |  0.8889 | 0.958  | 0.3043 |
+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+--------+--------+

{'BLEU-1': 0.7, 'BLEU-2': 0.6236095644623235, 'BLEU-3': 0.5297521706139517, 'BLEU-4': 0.4518010018049224, 'ROUGE-1': 0.888888888888889, 'ROUGE-2': 0.8571428571428571, 'ROUGE-3': 0.8, 'ROUGE-4': 0.6666666666666666, 'ROUGE-L': 0.888888888888889, 'METEOR': 0.9579668787425148, 'TER': 0.30434782608695654}
```